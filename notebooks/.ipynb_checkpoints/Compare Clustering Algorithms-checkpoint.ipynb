{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm\n",
    "import annoy\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from helpers import KeyTree\n",
    "from collections import Counter\n",
    "import cvxpy\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import linprog\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context='talk',font_scale=0.7)\n",
    "\n",
    "\n",
    "from hdbscan._hdbscan_tree import (condense_tree,\n",
    "                            compute_stability,\n",
    "                            get_clusters,\n",
    "                            outlier_scores,\n",
    "                                   do_labelling,\n",
    "                                   get_probabilities,\n",
    "                                   get_stability_scores)\n",
    "from hdbscan.hdbscan_ import _hdbscan_boruvka_kdtree, _hdbscan_prims_kdtree, _hdbscan_generic\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/dshiebler/workspace/personal/Category_Theory/unsupervised/hdbscan\")\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:355: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 11000. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP \n",
    "sampling = 20\n",
    "path = '/Users/dshiebler/workspace/personal/Category_Theory/FunctorialManifoldLearning/data/fashion'\n",
    "\n",
    "rawX, rawy = load_mnist(path, kind='train')\n",
    "untransformedX, y = rawX[::sampling], rawy[::sampling]\n",
    "X = UMAP(n_components=10).fit_transform(untransformedX)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ called (1)\n",
      "Checking algorithm\n",
      "calling _hdbscan_boruvka_kdtree\n",
      "Completed in 0.15660405158996582 seconds\n",
      "logreg:  0.5656845761313485\n",
      "hdbscan:  0.06256990580086404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "logreg_labels = clf.predict(X)\n",
    "hdbscan_labels = HDBSCAN(alpha=0.1).fit_predict(X)\n",
    "\n",
    "print(\"logreg: \", adjusted_rand_score(y, logreg_labels))\n",
    "print(\"hdbscan: \", adjusted_rand_score(y, hdbscan_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hdbscan_labels < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using approximate solution\n",
      "mhdbscan:  0.035406022301210906\n",
      "Counter({-1: 985, 67: 51, 6: 51, 27: 46, 19: 42, 57: 36, 65: 35, 76: 32, 44: 32, 22: 32, 15: 31, 92: 30, 93: 30, 23: 30, 42: 29, 56: 28, 83: 26, 52: 26, 50: 25, 35: 25, 70: 25, 89: 24, 62: 24, 90: 24, 18: 24, 41: 24, 7: 24, 59: 24, 32: 24, 103: 24, 54: 23, 58: 23, 105: 22, 61: 22, 91: 21, 66: 21, 53: 21, 74: 21, 29: 21, 17: 20, 48: 20, 25: 20, 86: 20, 88: 20, 98: 19, 107: 19, 51: 19, 80: 19, 75: 18, 95: 18, 84: 17, 73: 17, 26: 17, 60: 17, 13: 17, 2: 17, 33: 16, 77: 16, 104: 16, 31: 16, 55: 16, 99: 16, 78: 16, 14: 15, 87: 15, 71: 15, 45: 15, 20: 15, 79: 15, 8: 14, 47: 14, 63: 14, 96: 14, 64: 14, 100: 14, 69: 14, 1: 14, 38: 14, 82: 14, 37: 13, 16: 13, 11: 13, 46: 13, 3: 13, 4: 13, 97: 13, 81: 12, 94: 12, 24: 11, 10: 11, 85: 11, 102: 11, 108: 11, 36: 11, 39: 11, 9: 10, 68: 10, 101: 10, 43: 10, 106: 10, 12: 9, 34: 9, 5: 7, 30: 7, 21: 4, 49: 4, 40: 2, 72: 1, 28: 1})\n"
     ]
    }
   ],
   "source": [
    "from cluster_combinations import MultiscaleHDBSCAN\n",
    "\n",
    "# TODO: Why are there certain cases where this slows down massively? What can we do to prevent this?\n",
    "\n",
    "\n",
    "mhdbscan = MultiscaleHDBSCAN(\n",
    "#     alpha_list=[0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0],\n",
    "#     alpha_list=[1.0, 5.0, 10.0, 20.0, 50.0],\n",
    "    alpha_list=[0.5, 1.0, 2.0, 5.0],\n",
    "    min_samples=10,\n",
    "    min_cluster_size=20,\n",
    "    normalize_stabilities=False,\n",
    "    use_exact_solution=False)\n",
    "\n",
    "\n",
    "mhdbscan_labels = mhdbscan.fit_predict(X)\n",
    "print(\"mhdbscan: \", adjusted_rand_score(y, mhdbscan_labels))\n",
    "print(Counter(mhdbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 1619, 1: 777, 0: 77, 2: 527})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from cluster_combinations import (\n",
    "    run_multiscale_hdbscan, combine_trees_into_matrix, get_stability_vector, combine_clusters\n",
    "    )\n",
    "\n",
    "_, condensed_tree_list, stabilities_list, labels_list = run_multiscale_hdbscan(\n",
    "    X=X,\n",
    "    alpha_list=[0.001, 0.01, 5.0])\n",
    "\n",
    "\n",
    "raw_cluster_point_matrix = combine_trees_into_matrix(condensed_tree_list)\n",
    "raw_stability_vector = get_stability_vector(stabilities_list, normalize=True)\n",
    "raw_source_indices = [[ix] for i, s in enumerate(stabilities_list) for ix in [i]*len(s)]\n",
    "\n",
    "cluster_point_matrix, stability_vector, source_indices = combine_clusters(\n",
    "    raw_cluster_point_matrix, raw_stability_vector, raw_source_indices)\n",
    "\n",
    "bool_cluster_point_matrix = np.array(cluster_point_matrix, dtype=np.bool)\n",
    "overlap_matrix = np.matmul(bool_cluster_point_matrix, bool_cluster_point_matrix.T)\n",
    "\n",
    "raw_overlap_x, raw_overlap_y = np.nonzero(overlap_matrix)\n",
    "selected_overlap = raw_overlap_x < raw_overlap_y\n",
    "overlap_x, overlap_y = raw_overlap_x[selected_overlap], raw_overlap_y[selected_overlap], \n",
    "\n",
    "# One row for each pair of clusters that overlap, with a 1 in the position of each cluster\n",
    "A = np.zeros((cluster_point_matrix.shape[0], cluster_point_matrix.shape[0]))\n",
    "A = A + np.eye(cluster_point_matrix.shape[0]) * cluster_point_matrix.shape[0]\n",
    "A[overlap_x, overlap_y] = 1\n",
    "A[overlap_y, overlap_x] = 1\n",
    "\n",
    "b = np.ones(A.shape[0])*cluster_point_matrix.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster_combinations import remove_overlapping_in_order\n",
    "\n",
    "solution = linprog(c=-stability_vector, A_ub=A, b_ub=b)\n",
    "# print(solution.x)\n",
    "out = np.round(solution.x) > 0\n",
    "# print(np.matmul(A, out))\n",
    "new_out = remove_overlapping_in_order(A=A, c=-stability_vector, out=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using approximate solution\n"
     ]
    }
   ],
   "source": [
    "from cluster_combinations import solve_zero_one_linear_program\n",
    "\n",
    "solution = solve_zero_one_linear_program(\n",
    "    c=-stability_vector, A=A, b=b, use_exact_solution=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start = time.time()\n",
    "# _hdbscan_boruvka_kdtree(X,\n",
    "#            tree_builder=\"annoy\",\n",
    "#            min_samples=5,\n",
    "#            alpha=0.05,\n",
    "#            metric='euclidean',\n",
    "#            p=None,\n",
    "#            leaf_size=3,\n",
    "#            gen_min_span_tree=False)\n",
    "# print(\"_hdbscan_boruvka_kdtree annoy\", time.time() - start)\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# _hdbscan_boruvka_kdtree(X,\n",
    "#            tree_builder=\"kd\",\n",
    "#            min_samples=5,\n",
    "#            alpha=0.05,\n",
    "#            metric='euclidean',\n",
    "#            p=None,\n",
    "#            leaf_size=3,\n",
    "#            gen_min_span_tree=False)\n",
    "# print(\"_hdbscan_boruvka_kdtree kd\", time.time() - start)\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# _hdbscan_prims_kdtree(X,\n",
    "#            min_samples=5,\n",
    "#            alpha=0.05,\n",
    "#            metric='euclidean',\n",
    "#            p=None,\n",
    "#            leaf_size=3,\n",
    "#            gen_min_span_tree=False)\n",
    "# print(\"_hdbscan_prims_kdtree\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
